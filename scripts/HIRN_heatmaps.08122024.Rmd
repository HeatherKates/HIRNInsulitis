---
title: "HIRN insulitis nCounter analysis"
author: "Heather Kates"
date: "08-12-2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE,message = FALSE)
```

```{r Libraries,echo=FALSE,message=FALSE,warning=FALSE}
library(NanoTube)
library(dplyr)
library(ggplot2)
library(reshape2)
library(pheatmap)
library(knitr)
```

## Load the nCounter data

Load the data downloaded from the GeoMX DSP Initial_Dataset.xlsx and received from Zeina.

First we will load the data as-is and perform no automatic QC trimming or normalization

```{r process,warning=FALSE}
rawdata <- processNanostringData(nsFiles = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/data/HIRN_08122024_counts.csv",
                             sampleTab = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/data/HIRN_08122024_SampleData.csv",
                             idCol = "Custom_Segment_Name",
                             groupCol = "Segment_Tags",
                             normalization = "none")
```

### Quality Control

## Segment QC (housekeeping and background geomean)

The purpose of AOI-level QC is to identify AOIs with poor data that should be removed. We should look at both signal strength and background.

First, we will compute 2 metrics of AOI technical performance:
• Housekeeper geomean: this captures signal strength.
• IgG geomean: this captures background (negative controls), but in most experiments also reflects signal strength, as AOIs with more on-target signal also have more background.

```{r,QC_HK}
data <-rawdata

# Extract the necessary data
exprs_data <- exprs(data)
sample_data <- pData(data)

# Define the housekeeper genes names
hk_genes <- c("GAPDH", "S6", "Histone H3")

# Calculate the geometric mean for housekeeper genes
geometric_mean <- function(x) {
  # Remove NA values and non-positive values that would cause issues with log
  x <- x[x > 0]
  if (length(x) == 0) {
    return(NA)
  }
  exp(mean(log(x)))
}

hk_geomeans <- apply(exprs_data[hk_genes, ], 2, geometric_mean)

# Create a data frame for plotting
plot_data <- data.frame(
  Sample = colnames(exprs_data),
  HK_Geomean = log2(hk_geomeans),
  Scan_Name = sample_data$Scan_Name
)

# Plot the barplot with no x-axis labels and "Individual ROIs" as the x-axis title
ggplot(plot_data, aes(x = Sample, y = HK_Geomean, fill = Scan_Name)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(x = "Individual ROIs", y = "Housekeeper Geomean", fill = "Scan Name") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```


```{r QC_bd}
data <- rawdata

# Extract expression data
exprs_data <- exprs(data)
# Extract sample data
sample_data <- pData(data)

# Find the IgG genes by looking for "IgG" in the row names
igG_genes <- rownames(exprs_data)[grepl("IgG", rownames(exprs_data))]

# Function to calculate geometric mean
geometric_mean <- function(x) {
  # Remove NA values and non-positive values
  x <- x[x > 0]
  if (length(x) == 0) {
    return(NA)
  }
  exp(mean(log(x)))
}

# Calculate the geometric mean for IgG genes for each sample
igG_geomeans <- apply(exprs_data[igG_genes, ], 2, geometric_mean)

# Create a data frame for plotting
plot_data <- data.frame(
  Sample = colnames(exprs_data),
  IgG_Geomean = igG_geomeans,
  Scan_Name = sample_data$Scan_Name
)

# Create the bar plot for IgG genes
ggplot(plot_data, aes(x = Sample, y = IgG_Geomean, fill = Scan_Name)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(x = "Individual ROIs", y = "IgG Geomean", fill = "Scan Name") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

## Segment QC part II

Technical QC assessment includes FOV registration QC, Binding Density QC, Positive Control Normalization QC, Minimum nuclei count, Minimum surface area


```{r QC}
QC_data <- processNanostringData(nsFiles = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/data/HIRN_08122024_counts.csv",
                             sampleTab = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/data/HIRN_08122024_SampleData.csv",
                             idCol = "Custom_Segment_Name",
                             groupCol = "Segment_Tags",
                             normalization = "nSolver",
                             skip.housekeeping = TRUE,
                             output.format = "list")
```

```{r}
#add pc scale factors to sample data
QC_data[["samples"]]$pc_scalefactors <-  QC_data[["pc.scalefactors"]]

# Set the thresholds from the GeoMX DSP Analysis Suite
fov_threshold <- 75
binding_density_min <- 0.1
binding_density_max <- 2.25
pc_norm_min <- 0.3
pc_norm_max <- 3
nuclei_count_min <- 20
surface_area_min <- 1600

# Create QC flags
QC_data$samples$FOV_QC_Flag <- QC_data$samples$Fov_counted < fov_threshold
QC_data$samples$BindingDensity_QC_Flag <- with(QC_data$samples, BindingDensity < binding_density_min | BindingDensity > binding_density_max)
QC_data$samples$PCNorm_QC_Flag <- with(QC_data$samples, pc_scalefactors < pc_norm_min | pc_scalefactors > pc_norm_max)
QC_data$samples$NucleiCount_QC_Flag <- QC_data$samples$AOI_nuclei_count < nuclei_count_min
QC_data$samples$SurfaceArea_QC_Flag <- QC_data$samples$AOI_surface_area < surface_area_min
```

## Results of segment QC

```{r results='asis'}
# Assuming QC_data is your data frame with sample data and QC flags
# We'll filter the data for any rows with at least one TRUE flag
QC_flagged_data <- QC_data$samples %>%
  filter(FOV_QC_Flag | BindingDensity_QC_Flag | PCNorm_QC_Flag | NucleiCount_QC_Flag | SurfaceArea_QC_Flag) %>%
  select(Custom_Segment_Name, FOV_QC_Flag, BindingDensity_QC_Flag, PCNorm_QC_Flag, NucleiCount_QC_Flag, SurfaceArea_QC_Flag)

# Print the table using kable in R Markdown
kable(QC_flagged_data,row.names = FALSE)
#save to a list
QC_flagged_segments <- QC_flagged_data$Custom_Segment_Name
```

## Probe QC

Probes that never rise above background should be interpreted carefully. The plot below shows a  convenient way to identify poorly-performing probes. 

Here we compute and plot the “signal-tobackground” ratio per target, which is each AOI’s data divided by its IgG geomean.

```{r}
data <- rawdata

# Extract expression data
exprs_data <- exprs(data)

# Calculate the geometric mean for IgG controls
igG_controls <- rownames(exprs_data)[grepl("IgG", rownames(exprs_data))]
geometric_mean <- function(x) {
  x <- x[x > 0]
  if (length(x) == 0) {
    return(NA)
  }
  exp(mean(log(x)))
}
igG_geomeans <- apply(exprs_data[igG_controls, ], 2, geometric_mean)

# Compute signal-to-background ratio for each target
signal_to_background <- sweep(exprs_data, 2, igG_geomeans, FUN="/")
log2_signal_to_background <- log2(signal_to_background)

# Convert to long format for plotting
long_data <- melt(log2_signal_to_background)

# Order the features by whether they are IgG controls and their median signal-to-background ratio
features_ordered <- long_data %>%
  group_by(Var1) %>%
  summarize(is_igG = any(Var1 %in% igG_controls), median_value = median(value, na.rm = TRUE), .groups = 'drop') %>%
  arrange(is_igG, median_value) %>%
  pull(Var1)

features_ordered <- c(features_ordered[c(49,50,51)],features_ordered[c(1:48)])

# Update the variable factor levels to match the order
long_data$variable <- factor(long_data$Var1, levels = features_ordered)

# Create the boxplot
boxplot <- ggplot(long_data, aes(x = variable, y = value)) +
  geom_boxplot(outlier.shape = NA, color = "black") + # Black boxes without outliers
  geom_jitter(color = "red", width = 0.2, size = 0.5) + # All points in red with smaller size
  geom_hline(yintercept = 0, linetype = "solid") + # Add horizontal line at y=0
  geom_vline(xintercept = 3 + 0.5, linetype = "dashed") + # Correct vertical line position
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "", y = "Log2 Signal-to-Background Ratio")

# Print the plot
print(boxplot)
```


## Prune the dataset as needed

We will remove the four segments that failed segment QC. 

```{r}
#Define segments to keep
QC_passed_segments <- rawdata$Custom_Segment_Name[!rawdata$Custom_Segment_Name %in% QC_flagged_segments]
#data_filtered <- rawdata[,QC_passed_segments]
data_filtered <- rawdata
```

For now, we will leave all the probes in the dataset even though some have below background signal. We will make a list of these so that we can proceed with caution when evaluating results for these genes

### Normalization

Options for normalization include housekeeping, negative control normalization, background correction, and scale to area or nuclei count.

It is not recommended to use multiple of these options, but instead to explore the data and choose one approach to normalize.

## Explore IgG consistency  (Negative control normalization)

Do the IgG's have high enough counts to be used for normalization? 

These IgGs measure background, which can be a normalization method to compare slides and/or cell populations

```{r}
library(Biobase)

# Extract the necessary data
exprs_data <- exprs(data_filtered)
feature_data <- fData(data_filtered)
sample_data <- pData(data_filtered)


# Find the IgG control feature names
ig_controls <- rownames(feature_data)[grepl("IgG", rownames(feature_data))]

# Filter out only IgG controls data
ig_controls_data <- exprs_data[ig_controls, ]

# Create pairs of IgG controls for plotting
ig_pairs <- combn(ig_controls, 2, simplify = FALSE)

# Create a list to store ggplot objects
plots <- list()

# Loop over each pair and create a scatter plot
for (i in seq_along(ig_pairs)) {
    pair <- ig_pairs[[i]]
    
    # Prepare the data for plotting
    plot_data <- data.frame(
        #x = as.numeric(ig_controls_data[pair[1], ]),
        #y = as.numeric(ig_controls_data[pair[2], ]),
      x = ig_controls_data[pair[1], ],
        y = ig_controls_data[pair[2], ],
      #  Sample = colnames(ig_controls_data),
        Scan_Name=sample_data$Scan_Name
    )
    
    # Create the plot
    p <- ggplot(plot_data, aes(x = x, y = y)) +
        geom_point(aes(color = Scan_Name)) +
        geom_smooth(method = "lm", se = FALSE) +
        labs(x = pair[1], y = pair[2], title = paste("Scatter plot of", pair[1], "vs", pair[2])) +
        theme_minimal()
    
    # Add the plot to the list
    plots[[i]] <- p
}

# Print the plots
print(plots)
```


## Explore Housekeeper consistency (Housekeeping normalization)

Assumes the housekeepers measure primarily signal strength

Housekeepers should be highly correlated (with consistent ratios between them)

```{r}

# Extract the necessary data
exprs_data <- exprs(data_filtered)
feature_data <- fData(data_filtered)
sample_data <- pData(data_filtered)

# List of housekeeper genes
housekeepers <- c("GAPDH", "S6", "Histone H3")

# Ensure that the housekeeper genes are present in the dataset
if (!all(housekeepers %in% rownames(feature_data))) {
    stop("Not all housekeeper genes are present in the dataset.")
}

# Filter out only housekeeper genes data
hk_data <- exprs_data[housekeepers, ]

# Create pairs of housekeeper genes for plotting
hk_pairs <- combn(housekeepers, 2, simplify = FALSE)

# Create a list to store ggplot objects and correlation coefficients
plots <- list()
correlations <- data.frame(Pair = character(), R_squared = numeric(), stringsAsFactors = FALSE)

# Loop over each pair and create a scatter plot with a linear fit
for (i in seq_along(hk_pairs)) {
    pair <- hk_pairs[[i]]
    
    # Prepare the data for plotting
    plot_data <- data.frame(
        x = as.numeric(hk_data[pair[1], ]),
        y = as.numeric(hk_data[pair[2], ]),
        Sample = colnames(hk_data),
        Scan_Name=sample_data$Scan_Name
    )
    
    # Compute the correlation coefficient
    cor_coefficient <- cor(plot_data$x, plot_data$y)
    
    # Create the plot
    p <- ggplot(plot_data, aes(x = x, y = y)) +
        geom_point(aes(color = Scan_Name)) +
        geom_smooth(method = "lm", se = FALSE) +
        labs(x = pair[1], y = pair[2], title = paste("Scatter plot of", pair[1], "vs", pair[2]),
             subtitle = paste("R^2:", round(cor_coefficient^2, digits = 3))) +
        theme_minimal()
    
    # Add the plot and correlation to the lists
    plots[[i]] <- p
    correlations <- rbind(correlations, data.frame(Pair = paste(pair, collapse = " vs "), R_squared = cor_coefficient^2))
}

# Print the plots and correlations
print(plots)
print(correlations)
```


### Check for consistency between IgG's, housekeepers, area, nuclei counts

Check how the different variables perform with respect to each other before normalization. This also let's use assess whether scaling to area and nuclei counts is a viable approach.

```{r}
#define function to calculate geometric mean
geometric_mean <- function(x) {
  # Remove NA values and non-positive values that would cause issues with log
  x <- x[x > 0]
  if (length(x) == 0) {
    return(NA)
  }
  exp(mean(log(x)))
}

# Extract the necessary data
exprs_data <- exprs(data_filtered)
feature_data <- fData(data_filtered)
sample_data <- pData(data_filtered)

# Assuming geomeans for IgGs and housekeepers are calculated or can be accessed
# If not calculated, you would need to add code to calculate these from the exprs_data

# Access the area variable
area <- sample_data$AOI_surface_area
nuclei <- sample_data$AOI_nuclei_count

# Geomeans for IgGs
ig_controls <- rownames(feature_data)[grepl("IgG", rownames(feature_data))]
ig_geomean <- apply(exprs_data[ig_controls, ], 2, geometric_mean) # User needs to define geometric_mean or replace it with correct function

# Geomeans for housekeepers
housekeepers <- c("GAPDH", "S6", "Histone H3")
hk_geomean <- apply(exprs_data[housekeepers, ], 2, geometric_mean) # As above

# Create a combined data frame for plotting
consistency_data <- data.frame(
    Sample = colnames(exprs_data),
    IgG_Geomean = ig_geomean,
    HK_Geomean = hk_geomean,
    Area = area,
    Nuclei= nuclei,
    Scan_Name=sample_data$Scan_Name
)

# Plotting the relationships
p1 <- ggplot(consistency_data, aes(x = IgG_Geomean, y = HK_Geomean)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
  geom_point(aes(color = Scan_Name))+
    ggtitle("IgG vs Housekeeper Geomeans")

p2 <- ggplot(consistency_data, aes(x = HK_Geomean, y = Area)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
  geom_point(aes(color = Scan_Name))+
    ggtitle("Housekeeper Geomeans vs Area")

p3 <- ggplot(consistency_data, aes(x = IgG_Geomean, y = Area)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
  geom_point(aes(color = Scan_Name))+
    ggtitle("IgG Geomeans vs Area")

p4 <- ggplot(consistency_data, aes(x = IgG_Geomean, y = Nuclei)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
  geom_point(aes(color = Scan_Name))+
    ggtitle("IgG Geomeans vs Nuclei count")

p5 <- ggplot(consistency_data, aes(x = HK_Geomean, y = Nuclei)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
  geom_point(aes(color = Scan_Name))+
    ggtitle("Housekeeper Geomeans vs Nuclei count")

p6 <- ggplot(consistency_data, aes(x = Area, y = Nuclei)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10() +
    theme_minimal() +
  geom_point(aes(color = Scan_Name))+
    ggtitle("Area vs Nuclei count")

# Print the plots
print(p1)
print(p2)
print(p3)
print(p4)
print(p5)
print(p6)
```

Based on these results, we can consider HK normalization and scaling to nuceli count or area.

## Normalization

We will use housekeeping normalization with GAPDH and Histone H3

```{r process_norm,warning=FALSE}
#First we need to write the expression (including sample data) and feature data to csv files so we can re-process, becuase the next steps can only be done with a list or a new object
filt_counts <- read.csv("/blue/timgarrett/hkates/Campbell-Thompson/HIRN/data/HIRN_08122024_counts.csv")
filt_counts <- filt_counts %>% dplyr::select(c(colnames(filt_counts)[1:3], sample_data$Custom_Segment_Name))
filt_sample <- read.csv("/blue/timgarrett/hkates/Campbell-Thompson/HIRN/data/HIRN_08122024_SampleData.csv")
filt_sample <- filt_sample %>% dplyr::filter(Custom_Segment_Name %in% sample_data$Custom_Segment_Name)
write.csv(x = filt_counts, file="/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/filtered_counts.csv",row.names = FALSE)
write.csv(x=filt_sample,file="/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/filtered_sample_data.csv")
```

```{r,echo=TRUE}
#Process the filtered data with housekeeping normalization, bgType = "none" prevents the low signal probes from being removed. Background subtraction is still performed.

HKnorm_data <- processNanostringData(nsFiles = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/filtered_counts.csv",
                             sampleTab = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/filtered_sample_data.csv",
                             idCol = "Custom_Segment_Name",
                             groupCol = "Segment_Tags",
                             bgType="none",
                             normalization = "nSolver",
                             bgSubtract = TRUE,
                             housekeeping = c("GAPDH","Histone H3","S6"))
```

## View metadata

The variable used to assess heterogeneity was called "Segment_Tags" and has six levels.

```{r meta,echo=FALSE}
knitr::kable(table(HKnorm_data$Segment_Tags), 
             caption = "Distribution of Groups")
```

## View a PCA of ROIs colored by Scan_Name to assess batch effects

```{r PCA1,echo=FALSE}
data <- HKnorm_data
# Extract expression data
exprs_data <- data@assayData[["exprs"]]

# Perform PCA on the expression data, using transpose since prcomp expects samples as rows
pca_result <- prcomp(t(exprs_data), scale. = TRUE)

# Extract sample data
sample_data <- data@phenoData@data

# Ensure Scan_Name column exists in sample_data
if (!"Scan_Name" %in% names(sample_data)) {
  stop("The Scan_Name column does not exist in the phenoData of the ExpressionSet.")
}

# Create a data frame for plotting
pca_data <- data.frame(PC1 = pca_result$x[, 1],
                       PC2 = pca_result$x[, 2],
                       Scan_Name = sample_data$Scan_Name)

# Plot PCA by scan name
PCAp <- ggplot(pca_data, aes(x = PC1, y = PC2, color = Scan_Name)) +
  geom_point(alpha = 0.8, size = 3) +
  theme_minimal() +
  labs(title = "PCA of Expression Data", x = "PC1", y = "PC2") +
  scale_color_discrete(name = "Scan Name")

# Display the plot
print(PCAp)
rm(data)
```

## View a PCA of ROIs colored by variable of interest

```{r PCA2,echo=FALSE}
data <- HKnorm_data
# Plot PCA by group
# Create a data frame for plotting
pca_data <- data.frame(PC1 = pca_result$x[, 1],
                       PC2 = pca_result$x[, 2],
                       groups = sample_data$Segment_Tags)

PCAp <- ggplot(pca_data, aes(x = PC1, y = PC2, color = groups)) +
  geom_point(alpha = 0.8, size = 3) +
  theme_minimal() +
  labs(title = "PCA of Expression Data", x = "PC1", y = "PC2") +
  scale_color_discrete(name = "groups")
# Display the plot
print(PCAp)
# Update the PCA plot with better aesthetics
PCAprint <- PCAp + 
  theme_minimal(base_size = 16) +  # Set a minimal theme with a base font size
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18),  # Center and bold the title
    axis.title = element_text(face = "bold"),  # Bold axis titles
    axis.text = element_text(size = 14),  # Increase axis text size
    legend.title = element_text(face = "bold"),  # Bold the legend title
    legend.text = element_text(size = 14),  # Increase legend text size
    legend.position = "right",  # Position the legend on the right
    legend.background = element_rect(fill = "white", color = "black"),  # Add a border to the legend
    panel.grid.major = element_line(color = "grey80", size = 0.5),  # Adjust the grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    plot.background = element_rect(fill = "white"),  # Ensure a white background
    panel.border = element_blank()  # Remove the panel border
  ) +
  labs(
    title = "PCA of Expression Data",
    x = "PC1",
    y = "PC2",
    color = "Groups"
  ) +
  geom_point(size = 3)  # Make points larger and vary shapes by group

# Save the updated plot
ggsave("/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/PCA_plot.png", PCAprint, dpi = 300, width = 10, height = 8)
rm(data)
```

## Heatmaps with only proteins that passed background QC

```{r,echo=TRUE}
#Process the filtered data with housekeeping normalization, bgType = "threshold"
HKnorm_bg_data <- processNanostringData(nsFiles = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/filtered_counts.csv",
                             sampleTab = "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/filtered_sample_data.csv",
                             idCol = "Custom_Segment_Name",
                             groupCol = "Segment_Tags",
                             bgType="threshold",
                             normalization = "nSolver",
                             bgSubtract = TRUE,
                             housekeeping = c("GAPDH","Histone H3","S6"))
```

```{r,eval=FALSE}
Subset_bg_data <- HKnorm_bg_data[,pData(HKnorm_bg_data)$Segment %in% c("CD45","Others")]
```

### We can look at a heatmap of row-scaled data where the heatmap displays how each sample's expression level of a given protein compares to the average expression level of that protein across all samples. 

Therefore, the color in the heatmap indicates how much higher or lower the expression is relative to the protein's own average, not the absolute expression levels. Note that in row-scaled data, you cannot compare the relative levels of a protein within a sample.

```{r QCScaledHeatmap,echo=FALSE,fig.dim=c(10,6),eval=FALSE}
data <- HKnorm_bg_data
# Extract expression data for the genes of interest
genes_to_plot <- rownames(data@featureData@data %>% filter(CodeClass=="Endogenous"))
exprs_data <- data@assayData[["exprs"]]
genes_data <- data[genes_to_plot, ]

# Check if genes_to_plot actually exist in the data
missing_genes <- genes_to_plot[!genes_to_plot %in% rownames(exprs_data)]
if(length(missing_genes) > 0) {
  warning("The following genes are missing in the dataset and will be skipped:", paste(missing_genes, collapse = ", "))
}

# Extract sample data and determine order based on groups
sample_data <- data@phenoData@data
samples_order <- order(sample_data$Segment)

# Prepare the data matrix for the heatmap, possibly normalizing or transforming as necessary
heatmap_data <- genes_data[, samples_order]

# Plot the heatmap
# Note: Adjust the parameters of pheatmap as needed for your specific dataset and visualization preferences
pheatmap(heatmap_data,
         scale = "row", # Scale proteins to have 0 mean and 1 variance
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         cluster_cols = FALSE,
         cluster_rows=TRUE,
         color = colorRampPalette(c("blue", "white", "red"))(255), # Color gradient: adjust as needed
         show_rownames = TRUE,
         show_colnames = TRUE, # Change to TRUE if you want to show sample names
         annotation_col = sample_data[samples_order,"Segment_Tags", drop = FALSE],
         fontsize_col = 10,
         fontsize_row = 10,# Adjust as needed for sample annotations
main="row-scaled levels of QC-passed proteins across all segments")
rm(data)
```

## Heatmap of all protein expression for the normalized segments

```{r}
#Subset CD45 segments
#Subset_data <- HKnorm_data[,pData(HKnorm_data)$Segment=="CD45"]
Subset_data <- HKnorm_bg_data
```

```{r AllHeatmap,echo=FALSE,fig.dim=c(10,6)}
data <- Subset_data

# Extract expression data for the genes of interest
genes_to_plot <- rownames(data@featureData@data %>% filter(CodeClass=="Endogenous"))
exprs_data <- data@assayData[["exprs"]]
genes_data <- data[genes_to_plot, ]

# Check if genes_to_plot actually exist in the data
missing_genes <- genes_to_plot[!genes_to_plot %in% rownames(exprs_data)]
if(length(missing_genes) > 0) {
  warning("The following genes are missing in the dataset and will be skipped:", paste(missing_genes, collapse = ", "))
}

# Extract sample data and determine order based on groups
sample_data <- data@phenoData@data
samples_order <- order(sample_data$Segment)

# Prepare the data matrix for the heatmap, possibly normalizing or transforming as necessary
heatmap_data <- genes_data[, samples_order]
#log transform
# Extract the expression matrix from the ExpressionSet
exprs_data <- exprs(heatmap_data)
# Apply log transformation. Adding a small constant to avoid log of zero issues.
exprs_data_log <- log1p(exprs_data)


# Median Centering of Rows
median_centered_data <- t(apply(exprs_data_log, 1, function(x) x - median(x, na.rm = TRUE)))

# Set file path
output_file <- "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/heatmap_high_res.png"

# Define the dimensions (width and height) based on the number of rows and columns
width <- 10 + (90 * 0.2)  # Adjust width for 90 columns
height <- 10 + (35 * 0.5) # Adjust height for 35 rows

# Create the heatmap with a multi-line title
pheatmap(median_centered_data,
         scale = "none", # Scaling already done manually
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         cluster_cols = TRUE,
         cluster_rows = TRUE,
         color = colorRampPalette(c("blue", "white", "red"))(255),
         show_rownames = TRUE,
         show_colnames = FALSE,
         annotation_col = sample_data[samples_order, "Segment_Tags", drop = FALSE],
         fontsize_col = 12,  # Increase column annotation font size if needed
         fontsize_row = 14,  # Increase row label font size
         fontsize = 20,      # Increase general font size (affects legend, etc.)
         cellheight = 20,     # Adjust cell height to make rows shorter
         main = "Median-centered log() expression levels of filtered proteins across all ROIs\n(heatmap without dendrograms)",
         filename = output_file,  # Saving directly to a file
         width = 20,           # Adjust these parameters as needed
         height = 10)

pheatmap(median_centered_data,
         scale = "none",
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         cluster_cols = TRUE,
         cluster_rows = TRUE,
         color = colorRampPalette(c("blue", "white", "red"))(255),
         show_rownames = TRUE,
         show_colnames = FALSE,
         annotation_col = sample_data[samples_order, "Segment_Tags", drop = FALSE],
         fontsize_col = 12,
         fontsize_row = 14,
         fontsize = 20,
         cellheight = 20,
         main = "Median-centered log() expression levels of filtered proteins across all ROIs\n(heatmap without dendrograms)",
         width = 20,
         height = 10)
```

## Download housekeeping (GAPDH and Histone H3) normalized data

* Quality control was performed using default thresholds from the GeoMX DSP Analysis suite:

  +fov_threshold <- 75
  +binding_density_min <- 0.1
  +binding_density_max <- 2.25
  +pc_norm_min <- 0.3
  +pc_norm_max <- 3
  +nuclei_count_min <- 20
  +surface_area_min <- 1600

* Background subtraction was performed on all counts, but background removal was not performed (i.e. probes with low signal-to-background were **not** removed). No other background correction was performed.

* Housekeeping normalization was performed using housekeeping probes GAPHD and Histone H3

```{r Download,echo=FALSE,message=FALSE,warning=FALSE}
library(downloadthis)
HKnormdownload <- list(
  data.frame(HKnorm_data@assayData[["exprs"]]),
  data.frame(HKnorm_data@phenoData@data),
  data.frame(HKnorm_data@featureData@data))

names(HKnormdownload) <- c("Normalized_Counts","SampleData","FeatureData")

HKnormdownload$Normalized_Counts$Feature <- rownames(HKnormdownload$Normalized_Counts)

HKnormdownload$Normalized_Counts <- HKnormdownload$Normalized_Counts %>% relocate(Feature)

HKnormdownload %>% 
  download_this(
    output_name = "GeoMX_nCounter.HKnormalized",
    output_extension = ".xlsx",
    button_label = "Download HK normalized expression data and metadata",
    button_type = "default",
    has_icon = TRUE,
    icon = "fa fa-save"
  )
```

```{r,eval=FALSE}
library(openxlsx)
library(dplyr)

# Create the list of data frames
HKnormdownload <- list(
  data.frame(HKnorm_data@assayData[["exprs"]]),
  data.frame(HKnorm_data@phenoData@data),
  data.frame(HKnorm_data@featureData@data)
)

# Name the data frames in the list
names(HKnormdownload) <- c("Normalized_Counts", "SampleData", "FeatureData")

# Add Feature column to Normalized_Counts
HKnormdownload$Normalized_Counts$Feature <- rownames(HKnormdownload$Normalized_Counts)
HKnormdownload$Normalized_Counts <- HKnormdownload$Normalized_Counts %>% relocate(Feature)

# Define the file path to save the Excel file
output_path <- "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/GeoMX_nCounter.HKnormalized.xlsx"

# Save the list of data frames as an Excel file
wb <- createWorkbook()

# Loop through the list and add each data frame as a sheet in the Excel file
for (sheet_name in names(HKnormdownload)) {
  addWorksheet(wb, sheet_name)
  writeData(wb, sheet = sheet_name, x = HKnormdownload[[sheet_name]])
}

# Save the workbook to the specified path
saveWorkbook(wb, output_path, overwrite = TRUE)
```

## Download housekeeping (GAPDH and Histone H3) and background removed normalized data

* Quality control was performed using default thresholds from the GeoMX DSP Analysis suite:

  +fov_threshold <- 75
  +binding_density_min <- 0.1
  +binding_density_max <- 2.25
  +pc_norm_min <- 0.3
  +pc_norm_max <- 3
  +nuclei_count_min <- 20
  +surface_area_min <- 1600

* Background subtraction was performed on all counts, and background removal was performed to remove probes with low signal-to-background. No other background correction was performed.

* Housekeeping normalization was performed using housekeeping probes GAPHD and Histone H3

```{r Download2,echo=FALSE,message=FALSE,warning=FALSE}
library(downloadthis)
download <- list(
  data.frame(HKnorm_bg_data@assayData[["exprs"]]),
  data.frame(HKnorm_bg_data@phenoData@data),
  data.frame(HKnorm_bg_data@featureData@data))

names(download) <- c("Normalized_Counts","SampleData","FeatureData")

download$Normalized_Counts$Feature <- rownames(download$Normalized_Counts)

download$Normalized_Counts <- download$Normalized_Counts %>% relocate(Feature)

download %>% 
  download_this(
    output_name = "GeoMX_nCounter.HKnormalized.bgRemoved",
    output_extension = ".xlsx",
    button_label = "Download HK normalized bg removed expression data and metadata",
    button_type = "default",
    has_icon = TRUE,
    icon = "fa fa-save"
  )
```

```{r,eval=FALSE}
library(openxlsx)
library(dplyr)

# Create the list of data frames
download <- list(
  data.frame(HKnorm_bg_data@assayData[["exprs"]]),
  data.frame(HKnorm_bg_data@phenoData@data),
  data.frame(HKnorm_bg_data@featureData@data)
)

# Name the data frames in the list
names(download) <- c("Normalized_Counts", "SampleData", "FeatureData")

# Add Feature column to Normalized_Counts
download$Normalized_Counts$Feature <- rownames(download$Normalized_Counts)
download$Normalized_Counts <- download$Normalized_Counts %>% relocate(Feature)

# Define the file path to save the Excel file
output_path <- "/blue/timgarrett/hkates/Campbell-Thompson/HIRN/results/GeoMX_nCounter.HKnormalized.bgRemoved.xlsx"

# Save the list of data frames as an Excel file
wb <- createWorkbook()

# Loop through the list and add each data frame as a sheet in the Excel file
for (sheet_name in names(download)) {
  addWorksheet(wb, sheet_name)
  writeData(wb, sheet = sheet_name, x = download[[sheet_name]])
}

# Save the workbook to the specified path
saveWorkbook(wb, output_path, overwrite = TRUE)
```

